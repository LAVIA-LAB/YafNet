import os
os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'
import cv2
import numpy as np
import matplotlib.pyplot as plt
import glob
import pandas as pd
import seaborn as sns
import itertools
import random
import keras
import tensorflow as tf
import warnings

import sklearn
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score, roc_auc_score
from keras.models import load_model
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.utils import to_categorical
from sklearn.utils.class_weight import compute_class_weight

from keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

warnings.filterwarnings("ignore", category=DeprecationWarning)

# Set font size for plots
font_size = 16

# Set rcParams for better plot aesthetics
plt.rcParams.update({
    'lines.linewidth': 3.0,
    'axes.linewidth': 1.25,
    'axes.titlesize': font_size,
    'axes.labelsize': font_size,
    'xtick.labelsize': font_size - 2,
    'ytick.labelsize': font_size - 2,
    'legend.fontsize': font_size - 2,
    'xtick.major.size': 4,
    'xtick.major.width': 1.2,
    'ytick.major.size': 4,
    'ytick.major.width': 1.2,
    'axes.prop_cycle': plt.cycler(color=['tab:blue', 'tab:red', 'black'])
})

# Set the directories for training and validation data
train_dir = "ICDAR2021_Dataset_train_directory"
test_dir = "ICDAR2021_Dataset_test_directory"

# Load and sort script names
scripts_list = sorted(os.listdir(train_dir))
print('scripts:\n', *scripts_list, sep='\n  ')

num_classes = len(scripts_list)
print('Number of classes: ', num_classes)

# Set the image size and number of channels
img_rows, img_cols, img_depth = 112, 112, 1

# Display sample images from each script
fig = plt.figure(0, figsize=(15, 20))
index = 1
for script in scripts_list:
    script_path = os.path.join(train_dir, script)
    file_numbers = os.listdir(script_path)
    random_files = np.random.choice(file_numbers, size=5, replace=False)
    
    for i, random_file in enumerate(random_files):
        img = cv2.imread(os.path.join(script_path, random_file), cv2.IMREAD_GRAYSCALE)
        _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
        img = cv2.resize(img, (img_rows, img_cols))
        plt.subplot(num_classes, 5, index)
        plt.imshow(img, cmap='gray')
        plt.axis('off')
        if i == 2:
            plt.title(script.title(), color='orangered', weight='bold')
        index += 1

plt.show()

# Print number of images in training and test sets
print('Number of images in folders:')
print('Train: ', len(glob.glob(os.path.join(train_dir, "*/*"))))
print('Validation: ', len(glob.glob(os.path.join(test_dir, "*"))))

# Create a DataFrame for script counts and percentages
df = pd.DataFrame({
    'script': [script.title() for script in scripts_list],
    'Count': [len(os.listdir(os.path.join(train_dir, script))) for script in scripts_list]
})
df['Percentage'] = 100 * df['Count'] / df['Count'].sum()
df.sort_values(by='Count', ascending=False, inplace=True)

# Plot distribution of scripts in training set
fig, ax = plt.subplots(figsize=(8, 5), constrained_layout=True)
sns.barplot(y='script', x='Count', color='steelblue', edgecolor='tab:red', data=df)

for index, value in enumerate(df.index):
    label = np.round(df.iloc[index]['Percentage'], 1)
    ax.annotate(f'{label} %',
                xy=(df.iloc[index]['Count'] + 850, index),
                ha='center',
                va='center',
                color='tab:red',
                fontweight='bold',
                size=font_size - 1)

ax.set_title('Train Set - Distribution of Scripts')
ax.set_ylabel('')
ax.set_xlim(right=11000)
plt.show()

# Load training data
x_train, y_train = [], []
for index, script in enumerate(scripts_list):
    script_path = os.path.join(train_dir, script)
    for img_name in os.listdir(script_path):
        img = cv2.imread(os.path.join(script_path, img_name), cv2.IMREAD_GRAYSCALE)
        _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
        img = cv2.resize(img, (img_rows, img_cols))
        x_train.append(img.reshape(img_rows, img_cols, img_depth))
        y_train.append(index)

x_train = np.array(x_train, dtype=np.float32)
y_train1 = y_train
y_train = to_categorical(y_train, num_classes)

print("x_train shape: ", x_train.shape)
print("y_train shape: ", y_train.shape)

# Load test data
x_test, y_test = [], []

# Load test labels from files
file = open("ICDAR2021_Dataset\SIW_Groundtruth\TestCompetitionGroundtruth.txt", "r")
y_test = file.read()
file.close()
file = open("ICDAR2021_Dataset\SIW_Groundtruth\TestCompeititionTasks.txt", "r")
y_test1 = file.read()
file.close()

y_test = [int(x) for x in y_test.split()]
y_testph = [int(x) for x in y_test1.split()]
y_test1 = y_test

# Load test images
for img_name in os.listdir(test_dir):
    img = cv2.imread(os.path.join(test_dir, img_name), cv2.IMREAD_GRAYSCALE)
    _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)
    img = cv2.resize(img, (img_rows, img_cols))
    x_test.append(img.reshape(img_rows, img_cols, img_depth))
    
x_test = np.array(x_test, dtype=np.float32)
y_test = to_categorical(y_test, num_classes)

print("x_test shape: ", x_test.shape)
print("y_test shape: ", y_test.shape)

# Separate handwritten and printed test images
zero_indices = np.where(np.array(y_testph) == 0)[0]
one_indices = np.where(np.array(y_testph) == 1)[0]

x_test_h = x_test[zero_indices]
y_test_h = y_test[zero_indices]
x_test_p = x_test[one_indices]
y_test_p = y_test[one_indices]

print("x_test_h shape: ", x_test_h.shape)
print("y_test_h shape: ", y_test_h.shape)
print("x_test_p shape: ", x_test_p.shape)
print("y_test_p shape: ", y_test_p.shape)

# Normalize data
max_val = np.max(x_train)
x_train /= max_val
x_test /= max_val
x_test_h /= max_val
x_test_p /= max_val

index = 1
classnames = ['Arabic', 'Bangla', 'Gujrati', 'Gurmukhi', 'Hindi', 'Japanese', 'Kannada', 'Malayalam', 'Oriya', 'Roman','Tamil','Telugu','Thai']

# Display sample training images with labels
fig = plt.figure(0, figsize=(15, 20), constrained_layout=True)
idxs = np.random.randint(x_train.shape[0], size=5)

for i, j in enumerate(idxs):
    img = x_train[j]
    img = img.reshape(img_rows, img_cols, img_depth)
    label = np.argmax(y_train[j])
    plt.subplot(num_classes, 5, index)
    plt.imshow(img, cmap='gray')
    plt.axis('off')
    plt.title(scripts_list[label].title(), color='orangered', weight='bold')
    index += 1

plt.show()

# Convert y_train to integers for class labels
y_train_labels = np.argmax(y_train, axis=1)

# Compute class weights using integer labels
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)
class_weight_dict = dict(enumerate(class_weights))

# Define the model architecture
model = Sequential(name='CNN_Model')

### Block #01
model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(img_rows, img_cols, img_depth), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_01'))
model.add(BatchNormalization(name='BatchNorm_01'))
model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_02'))
model.add(BatchNormalization(name='BatchNorm_02'))
model.add(MaxPooling2D(pool_size=(2, 2), name='MaxPool2D_01'))

### Block #02
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_03'))
model.add(BatchNormalization(name='BatchNorm_03'))
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_04'))
model.add(BatchNormalization(name='BatchNorm_04'))
model.add(MaxPooling2D(pool_size=(2, 2), name='MaxPool2D_02'))

### Block #03
model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_05'))
model.add(BatchNormalization(name='BatchNorm_05'))
model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_06'))
model.add(BatchNormalization(name='BatchNorm_06'))
model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_07'))
model.add(BatchNormalization(name='BatchNorm_07'))
model.add(MaxPooling2D(pool_size=(2, 2), name='MaxPool2D_03'))

### Block #04
model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_08'))
model.add(BatchNormalization(name='BatchNorm_08'))
model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_09'))
model.add(BatchNormalization(name='BatchNorm_09'))
model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_10'))
model.add(BatchNormalization(name='BatchNorm_10'))
model.add(MaxPooling2D(pool_size=(2, 2), name='MaxPool2D_04'))

### Block #05
model.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_11'))
model.add(BatchNormalization(name='BatchNorm_11'))
model.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_12'))
model.add(BatchNormalization(name='BatchNorm_12'))
model.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_13'))
model.add(BatchNormalization(name='BatchNorm_13'))
model.add(MaxPooling2D(pool_size=(2, 2), name='MaxPool2D_05'))

### Dense  Layer
model.add(Flatten(name='Flatten'))

model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))
model.add(BatchNormalization(name='BatchNorm_14'))

### Output Layer
model.add(Dense(num_classes, activation='softmax', name='Output'))

model.summary()

batch_size=32
epochs=100

# Data augmentation for training data
train_datagen = ImageDataGenerator(
    rotation_range=5,
    #width_shift_range=0.04,
    #height_shift_range=0.04,
    shear_range=0.1,
    zoom_range=0.1,
    fill_mode='nearest'
)

# Create the training and validation generators
train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)

#ReduceLROnPlateau
learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.5, min_lr=1e-5)

# Compute the recall metric
def recall_m(y_true, y_pred):
    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))
    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))
    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())
    return recall

# Compute the precision metric
def precision_m(y_true, y_pred):
    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))
    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())
    return precision

# Compute the F1 score metric
def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))

# Compile the model
model.compile(loss='categorical_crossentropy', 
              optimizer=keras.optimizers.Adam(learning_rate=0.001, name='Adam'), 
              metrics=['accuracy', precision_m, recall_m, f1_m]
)

# Train the model
history = model.fit(
    train_generator,
    #x_train, y_train, batch_size,
    validation_data=(x_test, y_test),
    epochs = epochs,
    verbose = 1,
    callbacks = [learning_rate_reduction],
    class_weight=class_weight_dict
)

# Generate generalization metrics
score = model.predict(x_test)
scorep = model.predict(x_test_p)
scoreh = model.predict(x_test_h)

# Generate predictions and true labels
y_pred = np.argmax(score, axis=1)
y_true = np.argmax(y_test, axis=1)
y_pred_p = np.argmax(scorep, axis=1)
y_true_p = np.argmax(y_test_p, axis=1)
y_pred_h = np.argmax(scoreh, axis=1)
y_true_h = np.argmax(y_test_h, axis=1)

# Evaluate the model on the test data
residuals = np.argmax(score,1)!=np.argmax(y_test,1)
res = np.argmax(score,1)==np.argmax(y_test,1)
acc= sum(res)/len(res)
loss = sum(residuals)/len(residuals)
precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')
balanced_acc = balanced_accuracy_score(y_true, y_pred)
roc_auc = roc_auc_score(y_true, score, multi_class='ovo')  # For multiclass

# Print the results
print(f"Mixed Scripts Accuracy: {acc}")
print(f"Mixed Scripts Loss: {loss}")
print(f"Mixed Precision: {precision}")
print(f"Mixed Recall: {recall}")
print(f"Mixed F1 Score: {f1}")
print(f"Mixed Balanced Accuracy: {balanced_acc}")
print(f"Mixed ROC-AUC Score: {roc_auc}")

# Evaluate printed scripts
residualsp = np.argmax(scorep,1)!=np.argmax(y_test_p,1)
resp = np.argmax(scorep,1)==np.argmax(y_test_p,1)
accp= sum(resp)/len(resp)
lossp = sum(residualsp)/len(residualsp)
precision_p = precision_score(y_true_p, y_pred_p, average='weighted')
recall_p = recall_score(y_true_p, y_pred_p, average='weighted')
f1_p = f1_score(y_true_p, y_pred_p, average='weighted')
balanced_acc_p = balanced_accuracy_score(y_true_p, y_pred_p)
roc_auc_p = roc_auc_score(y_true_p, scorep, multi_class='ovo')  # For multiclass

print(f"Printed Scripts Accuracy: {accp}")
print(f"Printed Scripts Loss: {lossp}")
print(f"Printed Precision: {precision_p}")
print(f"Printed Recall: {recall_p}")
print(f"Printed F1 Score: {f1_p}")
print(f"Printed Balanced Accuracy: {balanced_acc_p}")
print(f"Printed ROC-AUC Score: {roc_auc_p}")

# Evaluate handwritten scripts
residualsh = np.argmax(scoreh,1)!=np.argmax(y_test_h,1)
resh = np.argmax(scoreh,1)==np.argmax(y_test_h,1)
acch= sum(resh)/len(resh)
lossh = sum(residualsh)/len(residualsh)
precision_h = precision_score(y_true_h, y_pred_h, average='weighted')
recall_h = recall_score(y_true_h, y_pred_h, average='weighted')
f1_h = f1_score(y_true_h, y_pred_h, average='weighted')
balanced_acc_h = balanced_accuracy_score(y_true_h, y_pred_h)
roc_auc_h = roc_auc_score(y_true_h, scoreh, multi_class='ovo')  # For multiclass

print(f"Handwritten Scripts Accuracy: {acch}")
print(f"Handwritten Scripts Loss: {lossh}")
print(f"Handwritten Precision: {precision_h}")
print(f"Handwritten Recall: {recall_h}")
print(f"Handwritten F1 Score: {f1_h}")
print(f"Handwritten Balanced Accuracy: {balanced_acc_h}")
print(f"Handwritten ROC-AUC Score: {roc_auc_h}")

# Mixed Confusion matrix

# Compute the normalized confusion matrix
cm = confusion_matrix(y_true, y_pred, normalize='true')

# Plot confusion matrix
fig, ax = plt.subplots(figsize=(10, 10))
im = ax.imshow(cm, interpolation='nearest', cmap='Reds')

# Set title and labels
ax.set(title='Normalized Confusion Matrix',
       xlabel='Predicted Label',
       ylabel='True Label')

# Set tick marks and labels
tick_marks = np.arange(len(scripts_list))
ax.set_xticks(tick_marks)
ax.set_xticklabels(scripts_list, rotation=45, ha='right')
ax.set_yticks(tick_marks)
ax.set_yticklabels(scripts_list)

# Annotate the confusion matrix
thresh = cm.max() / 2
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    ax.text(j, i, f"{cm[i, j]:.2f}",
            ha='center', va='center',
            color='white' if cm[i, j] > thresh else 'black')

# Add color bar
cax = plt.axes([0.95, 0.12, 0.05, 0.77])
plt.colorbar(im, cax=cax)

# Save and show plot
plt.savefig('Confusion_Matrix.svg')
plt.show()

# Printed Confusion matrix

# Compute the normalized confusion matrix
cm = confusion_matrix(y_true_p, y_pred_p, normalize='true')

# Plot confusion matrix
fig, ax = plt.subplots(figsize=(10, 10))
im = ax.imshow(cm, interpolation='nearest', cmap='Reds')

# Set title and labels
ax.set(title='Normalized Confusion Matrix for printed scripts',
       xlabel='Predicted Label',
       ylabel='True Label')

# Set tick marks and labels
tick_marks = np.arange(len(scripts_list))
ax.set_xticks(tick_marks)
ax.set_xticklabels(scripts_list, rotation=45, ha='right')
ax.set_yticks(tick_marks)
ax.set_yticklabels(scripts_list)

# Annotate the confusion matrix
thresh = cm.max() / 2
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    ax.text(j, i, f"{cm[i, j]:.2f}",
            ha='center', va='center',
            color='white' if cm[i, j] > thresh else 'black')

# Add color bar
cax = plt.axes([0.95, 0.12, 0.05, 0.77])
plt.colorbar(im, cax=cax)

# Save and show plot
plt.savefig('Confusion_Matrix.svg')
plt.show()

# Handwritten Confusion matrix

# Compute the normalized confusion matrix
cm = confusion_matrix(y_true_h, y_pred_h, normalize='true')

# Plot confusion matrix
fig, ax = plt.subplots(figsize=(10, 10))
im = ax.imshow(cm, interpolation='nearest', cmap='Reds')

# Set title and labels
ax.set(title='Normalized Confusion Matrix for handwritten scripts',
       xlabel='Predicted Label',
       ylabel='True Label')

# Set tick marks and labels
tick_marks = np.arange(len(scripts_list))
ax.set_xticks(tick_marks)
ax.set_xticklabels(scripts_list, rotation=45, ha='right')
ax.set_yticks(tick_marks)
ax.set_yticklabels(scripts_list)

# Annotate the confusion matrix
thresh = cm.max() / 2
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    ax.text(j, i, f"{cm[i, j]:.2f}",
            ha='center', va='center',
            color='white' if cm[i, j] > thresh else 'black')

# Add color bar
cax = plt.axes([0.95, 0.12, 0.05, 0.77])
plt.colorbar(im, cax=cax)

# Save and show plot
plt.savefig('Confusion_Matrix.svg')
plt.show()

warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")

# Plot training history
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), constrained_layout=True)

# Plot Loss
ax1.plot(history.history['loss'], label='Training Loss')
ax1.plot(history.history['val_loss'], label='Validation Loss')
ax1.set_title('Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend(loc='upper right')

# Plot Accuracy
ax2.plot(history.history['accuracy'], label='Training Accuracy')
ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')
ax2.set_title('Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.legend(loc='lower right')

for ax in [ax1, ax2]:
    ax.set_xlabel('Epoch')
    ax.set_xlim(left=-0.2, right=len(history.history['loss']) + 0.2)

plt.show()

# Function to predict a single image
def predict_single_img(file_name):
    img = x_test[file_name]
    img = np.array(img).reshape(-1, img_rows, img_cols, img_depth)
    img_t = np.expand_dims(img, axis=0)
    class_preds = model.predict(img_t)[0]
    return img, class_preds

# Function to plot wrong predictions
def plot_wrong_preds(num_of_imgs):
    file_names = np.random.choice(range(x_test.shape[0]), num_of_imgs, replace=False)

    for file_name in file_names:
        img, preds = predict_single_img(file_name)
        label = scripts_list[np.argmax(preds)]
        actual_label = scripts_list[y_true[file_name]]

        img = img.reshape(img_rows, img_cols, img_depth)

        if label != actual_label:
            fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

            clrs = ['grey' if x < max(preds) else 'tab:red' for x in preds]

            ax1.imshow(img)
            ax1.axis('off')

            sns.barplot(x=scripts_list, y=preds, palette=clrs, edgecolor='k', ax=ax2)
            ax2.set_title(f'Prediction: {label}', color='red')
            ax2.set_ylabel('Probability')

            plt.show()

# Display the ROC-AUC curve for a multi-class classification

# Binarize the labels for multi-class ROC
y_test_binarized = label_binarize(y_true, classes=range(num_classes))

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_binarized.ravel(), score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot all ROC curves
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'blue', 'yellow', 'purple', 'cyan', 'magenta', 'brown', 'pink', 'grey']

for i, color in zip(range(num_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {i} (area = {roc_auc[i]:0.2f})')

plt.plot(fpr["micro"], tpr["micro"], color='deeppink', linestyle=':', linewidth=4,
         label=f'micro-average ROC curve (area = {roc_auc["micro"]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Display the ROC-AUC curve for printed scripts

# Binarize the labels for multi-class ROC
y_test_p_binarized = label_binarize(y_true_p, classes=range(num_classes))

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc_p = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_p_binarized[:, i], scorep[:, i])
    roc_auc_p[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_p_binarized.ravel(), scorep.ravel())
roc_auc_p["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot all ROC curves
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'blue', 'yellow', 'purple', 'cyan', 'magenta', 'brown', 'pink', 'grey']

for i, color in zip(range(num_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {i} (area = {roc_auc_p[i]:0.2f})')

plt.plot(fpr["micro"], tpr["micro"], color='deeppink', linestyle=':', linewidth=4,
         label=f'micro-average ROC curve (area = {roc_auc_p["micro"]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for printed scripts')
plt.legend(loc="lower right")
plt.show()

# Display the ROC-AUC curve for handwritten scripts

# Binarize the labels for multi-class ROC
y_test_h_binarized = label_binarize(y_true_h, classes=range(num_classes))

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc_h = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_h_binarized[:, i], scoreh[:, i])
    roc_auc_h[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_h_binarized.ravel(), scoreh.ravel())
roc_auc_h["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot all ROC curves
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'blue', 'yellow', 'purple', 'cyan', 'magenta', 'brown', 'pink', 'grey']

for i, color in zip(range(num_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {i} (area = {roc_auc_h[i]:0.2f})')

plt.plot(fpr["micro"], tpr["micro"], color='deeppink', linestyle=':', linewidth=4,
         label=f'micro-average ROC curve (area = {roc_auc_h["micro"]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for handwritten scripts')
plt.legend(loc="lower right")
plt.show()
